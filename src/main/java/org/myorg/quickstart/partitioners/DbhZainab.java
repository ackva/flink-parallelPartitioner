package org.myorg.quickstart.partitioners;

        import org.apache.flink.api.common.JobExecutionResult;
        import org.apache.flink.api.common.functions.FilterFunction;
        import org.apache.flink.api.common.functions.MapFunction;
        import org.apache.flink.api.common.functions.Partitioner;
        import org.apache.flink.core.fs.FileSystem;
        import org.apache.flink.graph.Edge;
        import org.apache.flink.streaming.api.datastream.DataStream;
        import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
        import org.apache.flink.types.NullValue;
        import org.myorg.quickstart.utils.CustomKeySelector;
        import org.myorg.quickstart.utils.StoredObject;
        import org.myorg.quickstart.utils.StoredState;


import java.io.FileWriter;
        import java.io.IOException;
        import java.util.Random;
        import java.util.concurrent.TimeUnit;


/**
 * Created by zainababbas on 07/02/2017.
 */
public class DbhZainab {


    public static void main(String[] args) throws Exception {

        if (!parseParameters(args)) {
            return;
        }

        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);
        DataStream<Edge<Long, NullValue>> edges = getGraphStream(env);

        edges.partitionCustom(new DbhPartitioner<>(new CustomKeySelector(0),k), new CustomKeySelector<>(0)).writeAsCsv(outputPath, FileSystem.WriteMode.OVERWRITE).setParallelism(k);

        JobExecutionResult result = env.execute("My Flink Job");


/*        try {
            FileWriter fw = new FileWriter(log, true); //the true will append the new data
            fw.write("The job took " + result.getNetRuntime(TimeUnit.SECONDS) + " seconds to execute" + "\n");//appends the string to the file
            fw.write("The job took " + result.getNetRuntime(TimeUnit.NANOSECONDS) + " nanoseconds to execute" + "\n");
            fw.close();
        } catch (IOException ioe) {
            System.err.println("IOException: " + ioe.getMessage());
        }*/

    }
    private static String InputPath = null;
    private static String outputPath = null;
    private static String log = null;
    private static int k = 0;
    private static boolean parseParameters(String[] args) {

        if (args.length > 0) {
            if (args.length != 4) {
                System.err.println("Usage: Dbh <input edges path> <output path> <log> <partitions> ");
                return false;
            }

            InputPath = args[0];
            outputPath = args[1];
            log = args[2];
            k = (int) Long.parseLong(args[3]);
        } else {
            System.out.println("Executing example with default parameters and built-in default data.");
            System.out.println("  Provide parameters to read input data from files.");
            System.out.println(" Usage: Dbh <input edges path> <output path> <log> <partitions>");
        }
        return true;
    }



    ///////code for partitioner/////////
    private static class DbhPartitioner<T> implements Partitioner<T> {
        private static final long serialVersionUID = 1L;
        CustomKeySelector keySelector;

        private int k;
        StoredState currentState;
        private static final int MAX_SHRINK = 100;
        private double seed;
        private int shrink;

        public DbhPartitioner(CustomKeySelector keySelector, int k)
        {
            this.keySelector = keySelector;
            this.k= k;
            this.currentState = new StoredState(k);
            seed = Math.random();
            Random r = new Random();
            shrink = r.nextInt(MAX_SHRINK);

        }

        @Override
        public int partition(Object key, int numPartitions) {

            long target = 0L;
            try {
                target = (long) keySelector.getValue(key);
            } catch (Exception e) {
                e.printStackTrace();
            }

            long source = (long) key;


            int machine_id = -1;

            StoredObject first_vertex = currentState.getRecordOld(source);
            StoredObject second_vertex = currentState.getRecordOld(target);


            int shard_u = Math.abs((int) ( (int) source*seed*shrink) % k);
            int shard_v = Math.abs((int) ( (int) target*seed*shrink) % k);

            int degree_u = first_vertex.getDegree() +1;
            int degree_v = second_vertex.getDegree() +1;

            if (degree_v<degree_u){
                machine_id = shard_v;
            }
            else if (degree_u<degree_v){
                machine_id = shard_u;
            }
            else{ //RANDOM CHOICE
                //*** PICK A RANDOM ELEMENT FROM CANDIDATES
                Random r = new Random();
                int choice = r.nextInt(2);
                if (choice == 0){
                    machine_id = shard_u;
                }
                else if (choice == 1){
                    machine_id = shard_v;
                }
                else{
                    System.out.println("ERROR IN RANDOM CHOICE DBH");
                    System.exit(-1);
                }
            }
            //UPDATE EDGES
            Edge e = new Edge<>(source, target, NullValue.getInstance());
            currentState.incrementMachineLoad(machine_id,e);

            //UPDATE RECORDS
            if (currentState.getClass() == StoredState.class){
                StoredState cord_state = (StoredState) currentState;
                //NEW UPDATE RECORDS RULE TO UPFDATE THE SIZE OF THE PARTITIONS EXPRESSED AS THE NUMBER OF VERTICES THEY CONTAINS
                if (!first_vertex.hasReplicaInPartition(machine_id)){ first_vertex.addPartition(machine_id); cord_state.incrementMachineLoadVertices(machine_id);}
                if (!second_vertex.hasReplicaInPartition(machine_id)){ second_vertex.addPartition(machine_id); cord_state.incrementMachineLoadVertices(machine_id);}
            }
            else{
                //1-UPDATE RECORDS
                if (!first_vertex.hasReplicaInPartition(machine_id)){ first_vertex.addPartition(machine_id);}
                if (!second_vertex.hasReplicaInPartition(machine_id)){ second_vertex.addPartition(machine_id);}
            }

            //3-UPDATE DEGREES

            //System.out.print("source"+source);
            //System.out.println("target"+target);
            //System.out.println("machineid"+machine_id);
            first_vertex.incrementDegree();
            second_vertex.incrementDegree();

            return machine_id;
        }



    }


    public static  DataStream<Edge<Long, NullValue>> getGraphStream(StreamExecutionEnvironment env) throws IOException {

        return env.readTextFile(InputPath)
                .filter(new FilterFunction<String>() {
                    @Override
                    public boolean filter(String value) throws Exception {
                        return !value.contains("%");
                    }
                })
                .map(new MapFunction<String, Edge<Long, NullValue>>() {
                    @Override
                    public Edge<Long, NullValue> map(String s) throws Exception {
                        String[] fields = s.replaceAll(","," ").split(" ");
                        long src = Long.parseLong(fields[0]);
                        long trg = Long.parseLong(fields[1]);
                        return new Edge<>(src, trg, NullValue.getInstance());
                    }
                });

    }

}